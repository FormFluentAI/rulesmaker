# Rules Maker - Project Structure Overview

## ğŸ“ Project Organization

Based on analysis of the `c4ai` (Crawl4AI) and `mlscraper` projects, I've created an improved and more structured architecture for Rules Maker that combines the best aspects of both projects.

## ğŸ—ï¸ Architecture Improvements

### **From Crawl4AI (c4ai):**
- âœ… **Modular Design**: Clear separation of concerns with dedicated modules
- âœ… **Async Support**: Built for high-performance async operations
- âœ… **Comprehensive Configuration**: Rich configuration system via Pydantic models
- âœ… **Strategy Pattern**: Pluggable strategies for different extraction methods
- âœ… **CLI Interface**: Professional command-line interface with Click
- âœ… **Template System**: Jinja2-based template engine for rule generation

### **From MLScraper:**
- âœ… **Machine Learning Focus**: ML-based content extraction capabilities
- âœ… **Training-Based Approach**: Learn from examples to improve extraction
- âœ… **Lightweight Core**: Simple, focused API design
- âœ… **Pattern Recognition**: Automatic pattern detection in documentation

### **Our Improvements:**
- âœ… **AI-Focused**: Specifically designed for AI coding assistant rules
- âœ… **Multiple Output Formats**: Support for Cursor, Windsurf, and custom formats
- âœ… **Documentation-Aware**: Specialized for documentation website patterns
- âœ… **Better Type Safety**: Full Pydantic models with proper validation
- âœ… **Enhanced Template System**: Rich template engine with custom filters

## ğŸ“‚ Directory Structure

```
rules-maker/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ rules_maker/
â”‚       â”œâ”€â”€ __init__.py           # Main package exports
â”‚       â”œâ”€â”€ version.py            # Version information
â”‚       â”œâ”€â”€ models.py             # Pydantic data models
â”‚       â”œâ”€â”€ utils.py              # Utility functions
â”‚       â”œâ”€â”€ cli.py                # Command-line interface
â”‚       â”‚
â”‚       â”œâ”€â”€ scrapers/             # Web scraping components
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py           # Base scraper class
â”‚       â”‚   â”œâ”€â”€ documentation_scraper.py    # Sync documentation scraper
â”‚       â”‚   â”œâ”€â”€ async_documentation_scraper.py  # Async scraper
â”‚       â”‚   â””â”€â”€ adaptive_documentation_scraper.py  # ML-enhanced scraper
â”‚       â”‚
â”‚       â”œâ”€â”€ extractors/           # Content extraction strategies
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py           # Base extractor class
â”‚       â”‚   â”œâ”€â”€ ml_extractor.py   # Machine learning extractor
â”‚       â”‚   â”œâ”€â”€ llm_extractor.py  # LLM-powered extractor
â”‚       â”‚   â””â”€â”€ structured_extractor.py  # Rule-based extractor
â”‚       â”‚
â”‚       â”œâ”€â”€ transformers/         # Content transformation
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ base.py           # Base transformer class
â”‚       â”‚   â”œâ”€â”€ rule_transformer.py      # Generic rule transformer
â”‚       â”‚   â”œâ”€â”€ cursor_transformer.py    # Cursor rules format
â”‚       â”‚   â”œâ”€â”€ windsurf_transformer.py  # Windsurf rules format
â”‚       â”‚   â””â”€â”€ workflow_transformer.py  # Workflow generator
â”‚       â”‚
â”‚       â”œâ”€â”€ templates/            # Template engine and templates
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â”œâ”€â”€ engine.py         # Jinja2 template engine
â”‚       â”‚   â””â”€â”€ templates/        # Template files
â”‚       â”‚       â”œâ”€â”€ cursor_rules.j2      # Cursor rules template
â”‚       â”‚       â”œâ”€â”€ windsurf_rules.j2    # Windsurf rules template
â”‚       â”‚       â””â”€â”€ workflow.j2          # Workflow template
â”‚       â”‚
â”‚       â”œâ”€â”€ processors/           # Content processors
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ base.py           # Base processor (placeholder)
â”‚       â”‚
â”‚       â”œâ”€â”€ strategies/           # Strategy pattern implementations
â”‚       â”‚   â”œâ”€â”€ __init__.py
â”‚       â”‚   â””â”€â”€ base.py           # Base strategies (placeholder)
â”‚       â”‚
â”‚       â””â”€â”€ filters/              # Content filtering
â”‚           â”œâ”€â”€ __init__.py
â”‚           â””â”€â”€ base.py           # Base filters (placeholder)
â”‚
â”œâ”€â”€ tests/                        # Test suite
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ test_scrapers.py          # Basic scraper tests
â”‚
â”œâ”€â”€ examples/                     # Usage examples
â”‚   â”œâ”€â”€ basic_usage.py            # Basic Python API usage
â”‚   â””â”€â”€ cli_usage.py              # CLI usage examples
â”‚
â”œâ”€â”€ docs/                         # Documentation (future)
â”œâ”€â”€ config.example.yaml           # Example configuration
â”œâ”€â”€ requirements.txt              # Core dependencies
â”œâ”€â”€ requirements-dev.txt          # Development dependencies
â”œâ”€â”€ pyproject.toml               # Modern Python packaging
â”œâ”€â”€ LICENSE                      # MIT license
â””â”€â”€ README.md                    # This file
```

## ğŸ”§ Key Components

### **1. Scrapers Module**
- **Base Scraper**: Abstract base class with common functionality
- **Documentation Scraper**: Optimized for documentation websites
- **Async Scraper**: High-performance async scraping (future)
- **Adaptive Scraper**: ML-enhanced pattern recognition (future)

### **2. Extractors Module** 
- **Content Extractor**: Base class for content extraction
- **ML Extractor**: Machine learning-based extraction
- **LLM Extractor**: Large Language Model powered extraction
- **Structured Extractor**: Rule-based structured extraction

### **3. Transformers Module**
- **Rule Transformer**: Convert scraped content to rules
- **Cursor Transformer**: Generate .cursorrules files
- **Windsurf Transformer**: Generate Windsurf-compatible rules
- **Workflow Transformer**: Create workflow definitions

### **4. Templates Module**
- **Template Engine**: Jinja2-based rendering engine
- **Rule Templates**: Templates for different rule formats
- **Custom Filters**: Template filters for text processing

## ğŸš€ Usage Patterns

### **Python API**
```python
from rules_maker import DocumentationScraper, CursorRuleTransformer

# Configure and scrape
scraper = DocumentationScraper()
results = scraper.scrape_documentation_site("https://docs.example.com")

# Transform to rules
transformer = CursorRuleTransformer()
rules = transformer.transform(results)

# Save rules
with open(".cursorrules", "w") as f:
    f.write(rules)
```

### **CLI Interface**
```bash
# Simple scraping
rules-maker scrape https://docs.example.com --output .cursorrules

# Deep scraping with configuration
rules-maker scrape https://docs.example.com \
  --deep \
  --max-pages 50 \
  --format cursor \
  --output ./rules.txt

# Batch processing
rules-maker batch urls.txt --output-dir ./rules --format windsurf
```

## ğŸ”„ Comparison with Source Projects

| Feature | Crawl4AI | MLScraper | Rules Maker |
|---------|-----------|-----------|-------------|
| **Async Support** | âœ… Advanced | âŒ None | âœ… Planned |
| **ML Integration** | ğŸ”¶ Basic | âœ… Core Feature | âœ… Enhanced |
| **Documentation Focus** | ğŸ”¶ General Web | âŒ General | âœ… Specialized |
| **AI Rules Output** | âŒ Raw Content | âŒ Data Only | âœ… AI-Ready Rules |
| **Template System** | âŒ None | âŒ None | âœ… Jinja2-based |
| **CLI Interface** | âœ… Comprehensive | âŒ Limited | âœ… Feature-rich |
| **Type Safety** | âœ… Pydantic | ğŸ”¶ Basic | âœ… Full Pydantic |
| **Strategy Pattern** | âœ… Extensive | âŒ Limited | âœ… Simplified |

## ğŸ¯ Next Steps

1. **Complete Core Implementation**
   - Implement async scraper
   - Add ML-based extractors
   - Create LLM integration

2. **Enhanced Features**
   - Add more output formats
   - Implement learning strategies
   - Create GUI interface

3. **Testing & Documentation**
   - Comprehensive test suite
   - API documentation
   - Usage tutorials

4. **Performance Optimization**
   - Async/await optimization
   - Caching strategies
   - Memory management

This architecture provides a solid foundation that's both scalable and maintainable, incorporating the best practices from both reference projects while adding significant improvements for AI coding assistant integration.
